{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from transformers import pipeline\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2813 > 512). Running this sequence through the model will result in indexing errors\n",
      "Device set to use mps:0\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2813 > 512). Running this sequence through the model will result in indexing errors\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use mps:0\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2409 > 512). Running this sequence through the model will result in indexing errors\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use mps:0\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2409 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best HPO setting:\n",
      "{'model': 'facebook/bart-large-cnn', 'params': {'max_length': 200, 'min_length': 50}, 'rouge1_f': 0.3019220346049615, 'rouge2_f': 0.16872180451127822, 'rougeL_f': 0.2360322423737058}\n"
     ]
    }
   ],
   "source": [
    "def compute_rouge(reference, hypothesis):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    return scorer.score(reference, hypothesis)\n",
    "\n",
    "def load_examples(file_path, num_examples=5):\n",
    "    examples = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for _ in range(num_examples):\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            data = json.loads(line)\n",
    "            examples.append({\n",
    "                \"headline\": data.get(\"headline\", \"N/A\"),\n",
    "                \"human_summary\": data.get(\"summary\", \"N/A\"),\n",
    "                \"article\": data.get(\"text\", \"\")\n",
    "            })\n",
    "    return examples\n",
    "\n",
    "candidate_models = [\n",
    "    \"facebook/bart-large-cnn\",\n",
    "    \"t5-small\",\n",
    "    \"google/pegasus-xsum\"\n",
    "]\n",
    "param_grid = [\n",
    "    {\"max_length\": 150, \"min_length\": 40},\n",
    "    {\"max_length\": 200, \"min_length\": 50},\n",
    "]\n",
    "\n",
    "examples = load_examples(\"../data/newsroom/train.jsonl\", num_examples=5)\n",
    "\n",
    "results = []\n",
    "for model_name in candidate_models:\n",
    "    for params in param_grid:\n",
    "        summarizer = pipeline(\"summarization\", model=model_name)\n",
    "        scores_list = []\n",
    "        for ex in examples:\n",
    "            article = ex[\"article\"]\n",
    "            if \"t5\" in model_name:\n",
    "                article = \"summarize: \" + article\n",
    "            try:\n",
    "                generated = summarizer(article, max_length=params[\"max_length\"],\n",
    "                                       min_length=params[\"min_length\"], do_sample=False)\n",
    "                generated_summary = generated[0]['summary_text']\n",
    "            except Exception:\n",
    "                generated_summary = \"\"\n",
    "            scores_list.append(compute_rouge(ex[\"human_summary\"], generated_summary))\n",
    "        \n",
    "        avg_scores = {\n",
    "            \"model\": model_name,\n",
    "            \"params\": params,\n",
    "            \"rouge1_f\": np.mean([s['rouge1'].fmeasure for s in scores_list]),\n",
    "            \"rouge2_f\": np.mean([s['rouge2'].fmeasure for s in scores_list]),\n",
    "            \"rougeL_f\": np.mean([s['rougeL'].fmeasure for s in scores_list])\n",
    "        }\n",
    "        results.append(avg_scores)\n",
    "\n",
    "best = max(results, key=lambda x: x[\"rouge1_f\"])\n",
    "print(\"\\nBest HPO setting:\")\n",
    "print(best)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
